% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/prepare_logdata.R
\name{prepare_logdata}
\alias{prepare_logdata}
\title{Reads all JSON files into a single dataframe}
\usage{
prepare_logdata(
  path = "./",
  aggregate_events = FALSE,
  idle_time = 20,
  unzip = FALSE,
  event_codes = lucar::event_codes,
  tool_codes = lucar::tool_codes,
  debug_mode = FALSE
)
}
\arguments{
\item{path}{The path to the folder including all JSON files (files in subfolders are also considered).}

\item{aggregate_events}{If TRUE, the events with identical task codes and directly following each other will be collated to a single events}

\item{idle_time}{If not FALSE, it provides the time in seconds when an event is marked as idle - i.e. the participant is not doing anything.}

\item{unzip}{If true, the function looks for zip archives located in the given path, corresponding to the naming convention for exported data from LUCA Office, and unzips these.}

\item{event_codes}{Dataframe with the workflow coding that is used to structure the log data}

\item{tool_codes}{Dataframe with the tool coding that is used to assign each used tool to a common code}

\item{debug_mode}{If TRUE the results include the internal hash IDs for the project elements are included and a tibble including unknown event types (if there were any). If 'module_specific' is set to TRUE it will be enforced to 'FALSE'.}
}
\value{
A dataframe including the prepared data from all JSON files
}
\description{
JSON files exported from the LUCA office simulation into a dedicated folder
are read into a dataframe.
Best is to download all zip files into a dedicated folder and unpack them in that same folder.
Then provide this folder's path to the function.
}
\examples{

# Searches in the current working directory and all subdirectories for log data from LUCA office
# and prepares the data in a structure suitable for further analyses
\dontrun{
logdata <- prepare_logdata()
}

}
